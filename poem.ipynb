{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import random \n",
    "import urllib2\n",
    "import time\n",
    "from numpy.random import choice\n",
    "import numpy as np\n",
    "import sys\n",
    "#Uses 5-grams provided by Google\n",
    "#http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('as.txt','r')\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def atNoun4Adj(word): #find mostly nouns for adjective. \"as 'mysterious' as * Y\"\n",
    "    d={}\n",
    "    #cut to 10% to make faster\n",
    "    #i = iter(lines)\n",
    "    lines_cut = random.sample(lines, len(lines)/10)\n",
    "    for line in lines_cut: \n",
    "      if word in line:   #speeds up by 90% when not needing re.search for each line\n",
    "        if re.search('as ' +word+ ' as .* .*', line):\n",
    "            tokens = word_tokenize(line)\n",
    "            x = tokens[3]\n",
    "            y = tokens[4]\n",
    "            count = int(tokens[7])           \n",
    "            if y not in d:\n",
    "                 d[y]=0\n",
    "      #     if y not in d[x]:\n",
    "       #          d[x][y]=0\n",
    "            d[y]+=count                \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def atAdj4Noun(word): # find adjectives for a noun, \"as Y as * 'student'\"\n",
    "    d={}\n",
    "    #cut to 10% to make faster\n",
    "    #i = iter(lines)\n",
    "    lines_cut = random.sample(lines, len(lines)/10)\n",
    "    for line in lines_cut:\n",
    "      if word in line:  \n",
    "        if re.search('as .* as .* '+word, line):\n",
    "            tokens = word_tokenize(line)\n",
    "            x = tokens[1]\n",
    "            #y = tokens[4]\n",
    "            count = int(tokens[7])          \n",
    "            if x not in d:\n",
    "                 d[x]=0\n",
    "            d[x]+=count                \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copied from net, http://stackoverflow.com/questions/25714531/find-rhyme-using-nltk-in-python\n",
    "entries = nltk.corpus.cmudict.entries()\n",
    "def rhyme(inp, level):\n",
    "     syllables = [(word, syl) for word, syl in entries if word == inp]\n",
    "     rhymes = []\n",
    "     for (word, syllable) in syllables:\n",
    "             rhymes += [word for word, pron in entries if pron[-level:] == syllable[-level:]]\n",
    "     return set(rhymes)\n",
    "# this one is too slow as brute force through cmudict (130k words), better version would be finding\n",
    "# rhyming words as intersection of 2 sorted sets. Or by building hash version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copied from net, same: http://stackoverflow.com/questions/25714531/find-rhyme-using-nltk-in-python\n",
    "def doTheyRhyme( word1, word2 ):\n",
    "  # first, we don't want to report 'glue' and 'unglue' as rhyming words\n",
    "  # those kind of rhymes are LAME\n",
    "  if word1.find ( word2 ) == len(word1) - len ( word2 ):\n",
    "      return False\n",
    "  if word2.find ( word1 ) == len ( word2 ) - len ( word1 ): \n",
    "      return False\n",
    "  return word1 in rhyme ( word2, 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return a word and a dict of words rhyming with it\n",
    "def getRhyme(d, d2):\n",
    "    word1 = generate(d) #random item from dict d\n",
    "    keys_d2 = set(d2.keys())\n",
    "    all_rhyming = rhyme(word1, 1)\n",
    "    intersection = keys_d2 & all_rhyming\n",
    "    keys = intersection\n",
    "    #remove the word self from rhyming words\n",
    "    try:\n",
    "      keys.remove(word1)\n",
    "    except KeyError:\n",
    "      pass\n",
    "    #def extract(d, keys):\n",
    "    #    return dict((k, d[k]) for k in keys if k in d)\n",
    "    newdict2 =dict((k, d2[k]) for k in keys)\n",
    "    # if there would be no rhyming words, add one common rhyming\n",
    "    if len(newdict2.keys())==0:\n",
    "        word =str(random.sample(all_rhyming,1))\n",
    "        word=word[3:-2]\n",
    "        newdict2[word]=1 \n",
    "    return word1, newdict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(state):  #state is dict with keys and their sums, ie transitions and weights\n",
    "    #from operator import itemgetter\n",
    "    #sort(state, key=itemgetter(1))\n",
    "    elements = state.keys()\n",
    "    weights = state.values()\n",
    "    \n",
    "    together = zip(weights, elements)    \n",
    "    sorted_together = sorted(together, reverse=True)\n",
    "    weights= [x[0] for x in sorted_together]\n",
    "    elements = [x[1] for x in sorted_together]    \n",
    "    \n",
    "    #cut to max 200\n",
    "    if len(elements)>100:\n",
    "        elements=elements[0:100]\n",
    "        weights=weights[0:100]\n",
    "    \n",
    "    #print(len(elements)) \n",
    "        \n",
    "    total=float(0)\n",
    "    # from  counts, make probabilities to sum to 1\n",
    "    for index, item in enumerate(weights):\n",
    "        total+= item\n",
    "    for index, item in enumerate(weights):\n",
    "        weights[index] = item/total\n",
    "        \n",
    "    cmptotal=0\n",
    "    for index, item in enumerate(weights):\n",
    "        cmptotal+= item\n",
    "        if random.uniform(0, 1) < cmptotal:\n",
    "            return elements[index]\n",
    "\n",
    "    #suffix= np.random.choice(elements, p=weights)     \n",
    "    return suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with rhyme\n",
    "def makeLine(word1, t1,t2, fillwords):    \n",
    "    text = word1 +' '+ generate(fillwords) +' '+ t1 +' and '+word1 +' ' + generate(fillwords) +' '+ t2+'.'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with rhyme, for noun4(add4(noun))\n",
    "def makeLine2(word1, adj1 ,adj2, fillwords):    \n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "      text = 'As '+adj1 +' and as '+adj2 +' as '+ word1 +' '+ generate(fillwords)+'.'\n",
    "    else:\n",
    "      text = 'So '+adj1 +' and so '+adj2 +' as '+ word1 +' '+ generate(fillwords)+'.'    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWords(d, d2):    \n",
    "    ta1 = generate(d)\n",
    "    tb1 = generate(d2)    \n",
    "    ta2, dict2 = getRhyme(d, d2)\n",
    "    tb2= generate(dict2)   \n",
    "    # if 2 same, do 1st ones again\n",
    "    i=0\n",
    "    if ta1==ta2 and i<200:\n",
    "        d[ta1]=d[ta1]/2\n",
    "        ta1=generate(d)\n",
    "        i+=1    \n",
    "    i=0\n",
    "    if tb1==tb2 and i<200:\n",
    "        d2[tb1]=d2[tb1]/2        \n",
    "        tb1=generate(d2)\n",
    "        i+=1    \n",
    "    return ta1, ta2, tb1, tb2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fillwords = {}\n",
    "fillwords[\"is\"] = 50\n",
    "fillwords[\"was\"] = 40\n",
    "fillwords[\"will be\"] = 40\n",
    "fillwords[\"is so\"] = 30\n",
    "fillwords[\"is now\"] = 15\n",
    "fillwords[\"was always\"] = 10\n",
    "#usage generate(fillwords), returns 1 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processWords(word1, word2):\n",
    "    d = atAdj4Noun(word1)\n",
    "    d2 = atAdj4Noun(word2)\n",
    "    d['well']=1 #fix 'well' to smaller as its not correct context \"x as well as...\" x is not well.\n",
    "    d['long']=1\n",
    "    d['soon']=1\n",
    "    d['far']=1\n",
    "    d['much']=1\n",
    "    d['large']=1\n",
    "    d2['well']=1 \n",
    "    d2['long']=1 \n",
    "    d2['soon']=1 \n",
    "    d2['far']=1\n",
    "    d2['much']=1\n",
    "    d2['large']=1\n",
    "    return d, d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processWords2(word1, word2):\n",
    "    dA = atAdj4Noun(word1) #dictAdj\n",
    "    dB = atAdj4Noun(word2)\n",
    "    dA['well']=1\n",
    "    dA['soon']=1\n",
    "    dA['far']=1\n",
    "    dA['fast']=1\n",
    "    dA['long']=1\n",
    "    dA['much']=1    \n",
    "    dB['well']=1\n",
    "    dB['soon']=1\n",
    "    dB['far']=1\n",
    "    dB['fast']=1\n",
    "    dB['long']=1\n",
    "    dB['much']=1    \n",
    "    adj3 =  generate(dA)\n",
    "    adj4 =  generate(dA)\n",
    "    adj5 =  generate(dB)\n",
    "    adj6 =  generate(dB)\n",
    "    return adj3,adj4,adj5,adj6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reduce weight of common non relating words\n",
    "def fixWords(dB): #dict\n",
    "    dB['well']=1\n",
    "    dB['soon']=1\n",
    "    dB['far']=1\n",
    "    dB['fast']=1\n",
    "    dB['long']=1\n",
    "    dB['much']=1\n",
    "    return dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from net http://stackoverflow.com/questions/26320697/capitalization-of-each-sentence-in-a-string-in-python-3\n",
    "def sentenceCapitalizer(string1):\n",
    "    sentences = string1.split(\". \")\n",
    "    sentences2 = [sentence[0].capitalize() + sentence[1:] for sentence in sentences]\n",
    "    string2 = '. '.join(sentences2)\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPoem(word1='empty', word2='empty'):\n",
    "    if word1=='empty':\n",
    "        word1 = random.choice(tokens)\n",
    "    if word2=='empty':\n",
    "        word2 = random.choice(tokens)    \n",
    "    \n",
    "    t1,t2,t3,t4 = getWords(d, d2)\n",
    "    poem = makeLine(word1, t1,t2, fillwords) +' '\n",
    "    poem += makeLine(word2, t3,t4, fillwords)+' \\n'    \n",
    "    return sentenceCapitalizer(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPoem2(word1, word2):    \n",
    "    adj3, adj4, adj5,adj6 = processWords2(word1, word2)\n",
    "    dnoun3 = atNoun4Adj(adj3)\n",
    "    dnoun3 =fixWords(dnoun3)\n",
    "    noun3 = generate(dnoun3)\n",
    "    dnoun4 = atNoun4Adj(adj5)\n",
    "    dnoun4 =fixWords(dnoun4)\n",
    "    noun4 = generate(dnoun4)\n",
    "    poem = makeLine2(noun3, adj3, adj4, fillwords)+' '\n",
    "    poem+= makeLine2(noun4, adj5, adj6, fillwords)+' \\n'\n",
    "    return sentenceCapitalizer(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give 2 nouns for subjects of a poem.\n"
     ]
    }
   ],
   "source": [
    "print \"Give 2 nouns for subjects of a poem.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give a noun1 (substantive) \n",
      "man\n",
      "Give a noun2 \n",
      "woman\n"
     ]
    }
   ],
   "source": [
    "print \"Give a noun1 (substantive) \"\n",
    "word1 = raw_input()\n",
    "print \"Give a noun2 \"\n",
    "word2 = raw_input()\n",
    "d,d2 = processWords(word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man was high and man will be thick. Woman will be tender and woman will be weak. \n",
      "\n",
      "So good and so high as feast will be. So weak and so beautiful as child will be. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "poem = createPoem(word1,word2) \n",
    "print(poem)\n",
    "sys.stdout.flush()\n",
    "poem2 = createPoem2(word1, word2)\n",
    "print(poem2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
